---
title: "Discovery_dataset"
author: "Ariel-ac4391"
date: "11/22/2018"
output: pdf_document
---


\noindent Here, I recapitulate the main step related in the research paper with the graphs associated


\noindent The first step is data cleansing :

```{r}
training_data=read.csv("data/Data_User_Modeling_training_Dataset.csv")
test_data=read.csv("data/Data_User_Modeling_test_Dataset.csv")
library(gplots)
library(ggplot2)
library(partykit)
library(rpart) # Popular decision tree algorithm
library(hier.part)
library(dplyr)
library(ipred)
library(randomForest)
#library(rattle) # GUI for building trees and fancy tree plot #Doesn't work
library(rpart.plot) # Enhanced tree plots
library(party) # Alternative decision tree algorithm
library(partykit) # Convert rpart object to BinaryTree
#library(RWeka) # Weka decision tree J48.
library(C50) # Original C5.0 implementation.
library(e1071) # naive bayes
library(DMwR) # KNN
summary(training_data)
attach(training_data)
```

```{r}
# Number of distinct values in each feture
a = n_distinct(STG)
b = n_distinct(SCG)
c = n_distinct(STR)
d = n_distinct(LPR)
e = n_distinct(PEG)
f = n_distinct(UNS)
num_distinct = c(a,b,c,d,e,f)

plot = barplot(num_distinct, names = c("STG", "SCG", "STR", "LPR", "PEG", "UNS"), ylim=c(0,120), xlab="All Features", col=c("green", "purple", "orange", "yellow", "blue", "magenta"))
text(plot,num_distinct + 4,labels=as.character(num_distinct))
```

```{r}
# boxplot of all data
boxplot2(STG,SCG,STR,LPR,PEG, col=c("green", "purple", "orange", "yellow", "blue", "magenta"), ylim=c(0,1.2))

# boxplot of SCG divided accross UNS values
boxplot2(SCG~UNS,data=training_data, horizontal = TRUE,
  	 xlab="The degree of study time for goal materials", ylab="User Knowledge", col=c("green", "purple", "orange", "yellow", "blue"))

# boxplot of STG divided accross UNS values
boxplot2(STG~UNS,data=training_data, horizontal = TRUE,
  	 xlab="The degree of repetition number of user for goal materials", ylab="User Knowledge", col=c("green", "purple", "orange", "yellow", "blue"))


# boxplot of STR divided accross UNS values
boxplot2(STR~UNS,data=training_data, horizontal = TRUE,
  	 xlab="The degree of study time for related objects with goal materials", ylab="User Knowledge", col=c("green", "purple", "orange", "yellow", "blue"))

# boxplot of LPR divided accross UNS values
boxplot2(LPR~UNS,data=training_data, horizontal = TRUE,
  	 xlab="The exam performance of user for related objects with goal materials", ylab="User Knowledge", col=c("green", "purple", "orange", "yellow", "blue"))

# boxplot of PEG divided accross UNS values
boxplot2(PEG~UNS,data=training_data, horizontal = TRUE,
  	 xlab="The exam performance of user for goal materials", ylab="User Knowledge", col=c("green", "purple", "orange", "yellow", "blue"))
```

```{r}
# decision tree
tree1 <- ctree(UNS ~ .,data = training_data)
plot(tree1) #Review the design
fit1 = predict(tree1, test_data)
table(fit1, test_data$UNS)
```

```{r}
#recursive partition tree
tree2 <- rpart(UNS ~ ., data = training_data)
rpart.plot(tree2)
rpart.rules(tree2)
fit2 = predict(tree2, test_data, type = "class")
table(fit2, test_data$UNS)
```

```{r}
# J48 package issues
```

```{r}
# PART package issues
#hier.part(training_data$UNS, training_data)
```

```{r}
# Bagging tree NOTE: Interesting we did much better than them here, they did something wrong
tree3 = bagging(UNS ~., data=training_data, coob=TRUE)
fit3 = predict(tree3, test_data)
table(fit3, test_data$UNS)
```

```{r}
# Random Forest
tree4 = randomForest(UNS ~., data=training_data)
fit4 = predict(tree4, test_data)
table(fit4, test_data$UNS)
```

```{r}
# C5.0
tree5 <- C5.0(UNS ~., data=training_data)
fit5 = predict(tree5, test_data)
table(fit5, test_data$UNS)
```
```{r}
# naive bayes
bayes <- naiveBayes(UNS ~., data=training_data)
fit6 = predict(bayes, test_data)
table(fit6, test_data$UNS)

# Aggregate Data - Add accuracy to each model, compile, add missing algorithms if possible
```
```{r}
# knn
nn4 <- kNN(UNS ~ .,training_data,test_data,norm=FALSE,k=4)
table(test_data[,'UNS'],nn4)
```

```{r}
# SVM classification

```