---
title: "Discovery_dataset"
author: "Ariel-ac4391"
date: "11/22/2018"
output: pdf_document
---



```{r}
training_data=read.csv("data/Data_User_Modeling_training_Dataset.csv")
test_data=read.csv("data/Data_User_Modeling_test_Dataset.csv")
library(gplots)
library(ggplot2)
library(partykit)
library(rpart) # Popular decision tree algorithm
library(hier.part)
library(dplyr)
library(ipred)
library(randomForest)
#library(rattle) # GUI for building trees and fancy tree plot #Doesn't work
library(rpart.plot) # Enhanced tree plots
library(party) # Alternative decision tree algorithm
library(partykit) # Convert rpart object to BinaryTree
library(RWeka) # Weka decision tree J48.
library(C50) # Original C5.0 implementation.
library(e1071) # naive bayes and SVM
library(DMwR) # KNN
library(plotly)
summary(training_data)
attach(training_data)
```
```{r}
summary(training_data)
```

```{r}
# Number of distinct values in each feture
a = n_distinct(STG)
b = n_distinct(SCG)
c = n_distinct(STR)
d = n_distinct(LPR)
e = n_distinct(PEG)
f = n_distinct(UNS)
num_distinct = c(a,b,c,d,e,f)

plot = barplot(num_distinct, names = c("STG", "SCG", "STR", "LPR", "PEG", "UNS"), ylim=c(0,120), xlab="All Features", col=c("green", "purple", "orange", "yellow", "blue", "magenta"))
text(plot,num_distinct + 4,labels=as.character(num_distinct))
```

```{r}
# boxplot of all data
boxplot2(STG,SCG,STR,LPR,PEG, col=c("green", "purple", "orange", "yellow", "blue", "magenta"), ylim=c(0,1.2))

# boxplot of SCG divided accross UNS values
boxplot2(SCG~UNS,data=training_data, horizontal = TRUE,
  	 xlab="The degree of study time for goal materials", ylab="User Knowledge", col=c("green", "purple", "orange", "yellow", "blue"))

# boxplot of STG divided accross UNS values
boxplot2(STG~UNS,data=training_data, horizontal = TRUE,
  	 xlab="The degree of repetition number of user for goal materials", ylab="User Knowledge", col=c("green", "purple", "orange", "yellow", "blue"))


# boxplot of STR divided accross UNS values
boxplot2(STR~UNS,data=training_data, horizontal = TRUE,
  	 xlab="The degree of study time for related objects with goal materials", ylab="User Knowledge", col=c("green", "purple", "orange", "yellow", "blue"))

# boxplot of LPR divided accross UNS values
boxplot2(LPR~UNS,data=training_data, horizontal = TRUE,
  	 xlab="The exam performance of user for related objects with goal materials", ylab="User Knowledge", col=c("green", "purple", "orange", "yellow", "blue"))

# boxplot of PEG divided accross UNS values
boxplot2(PEG~UNS,data=training_data, horizontal = TRUE,
  	 xlab="The exam performance of user for goal materials", ylab="User Knowledge", col=c("green", "purple", "orange", "yellow", "blue"))
```
```{r}
#Independent variables Scatterplot
my_cols <- c("green", "purple", "orange", "yellow")
#pairs(~STG+SCG+STR+LPR+PEG, data=training_data, col = my_cols[training_data$UNS], upper.panel=NULL)

# Correlation panel
panel.cor <- function(x, y){
    usr <- par("usr"); on.exit(par(usr))
    par(usr = c(0, 1, 0, 1))
    r <- round(cor(x, y), digits=2)
    txt <- paste0("R = ", r)
    cex.cor <- 0.8/strwidth(txt)
    text(0.5, 0.5, txt, cex = cex.cor*r*3)
}
# Customize upper panel
upper.panel<-function(x, y){
  points(x,y, col = my_cols[training_data$UNS])
}
# Create the plots
pairs(~STG+SCG+STR+LPR+PEG, data=training_data, lower.panel = panel.cor, upper.panel = upper.panel)
```

```{r}
training_data$UNS <- as.factor(training_data$UNS)
p <- plot_ly(training_data, x = ~PEG, y = ~LPR, z = ~STG, color = ~UNS, colors = c("purple", "orange", "yellow", "green")) %>%
add_markers() %>%
layout(scene = list(xaxis = list(title = 'PEG'), yaxis = list(title = 'LPR'), zaxis = list(title = 'STG')))
p
```

```{r}
# decision tree
tree1 <- ctree(UNS ~ .,data = training_data)
#plot(tree1) #Review the design
fit1 = predict(tree1, test_data)
table = table(fit1, test_data$UNS)
table


n = sum(table) # number of instances
nc = nrow(table) # number of classes
diag = diag(table) # number of correctly classified instances per class 
rowsums = apply(table, 1, sum) # number of instances per class
colsums = apply(table, 2, sum) # number of predictions per class
p = rowsums / n # distribution of instances over the actual classes
q = colsums / n # distribution of instances over the predicted classes


precision = diag / colsums 
recall = diag / rowsums 
f1 = 2 * precision * recall / (precision + recall)

#The accuracy is:
accuracy = sum(diag) / n
accuracy

#Here is the performance metrics
data.frame(precision, recall, f1)
```

```{r}
#recursive partition tree
tree2 <- rpart(UNS ~ ., data = training_data)
rpart.plot(tree2)
rpart.rules(tree2)
fit2 = predict(tree2, test_data, type = "class")
table = table(fit2, test_data$UNS)
table


n = sum(table) # number of instances
nc = nrow(table) # number of classes
diag = diag(table) # number of correctly classified instances per class 
rowsums = apply(table, 1, sum) # number of instances per class
colsums = apply(table, 2, sum) # number of predictions per class
p = rowsums / n # distribution of instances over the actual classes
q = colsums / n # distribution of instances over the predicted classes


precision = diag / colsums 
recall = diag / rowsums 
f1 = 2 * precision * recall / (precision + recall)

#The accuracy is:
accuracy = sum(diag) / n
accuracy

#Here is the performance metrics
data.frame(precision, recall, f1)
```

```{r}
# J48 decision tree
j48 <- J48(UNS ~ ., data = training_data)
j48
write_to_dot(j48, con=stdout())
```

```{r}
# hierarchical partitioning
levels=c(1,2,3,4)
names(levels) = c("High", "Middle", "Low", "very_low")
IND_VARS <- subset(training_data, select = -UNS)
hier.part(levels[training_data$UNS], IND_VARS, family = "gaussian", gof = "RMSPE", barplot = TRUE)
```

```{r}
# Bagging tree NOTE: Interesting we did much better than them here, they did something wrong
tree3 = bagging(UNS ~., data=training_data, coob=TRUE)
fit3 = predict(tree3, test_data)
table = table(fit3, test_data$UNS)

table


n = sum(table) # number of instances
nc = nrow(table) # number of classes
diag = diag(table) # number of correctly classified instances per class 
rowsums = apply(table, 1, sum) # number of instances per class
colsums = apply(table, 2, sum) # number of predictions per class
p = rowsums / n # distribution of instances over the actual classes
q = colsums / n # distribution of instances over the predicted classes


precision = diag / colsums 
recall = diag / rowsums 
f1 = 2 * precision * recall / (precision + recall)

#The accuracy is:
accuracy = sum(diag) / n
accuracy

#Here is the performance metrics
data.frame(precision, recall, f1)
```

```{r}
# Random Forest
tree4 = randomForest(UNS ~., data=training_data)
fit4 = predict(tree4, test_data)
table = table(fit4, test_data$UNS)
table


n = sum(table) # number of instances
nc = nrow(table) # number of classes
diag = diag(table) # number of correctly classified instances per class 
rowsums = apply(table, 1, sum) # number of instances per class
colsums = apply(table, 2, sum) # number of predictions per class
p = rowsums / n # distribution of instances over the actual classes
q = colsums / n # distribution of instances over the predicted classes


precision = diag / colsums 
recall = diag / rowsums 
f1 = 2 * precision * recall / (precision + recall)

#The accuracy is:
accuracy = sum(diag) / n
accuracy

#Here is the performance metrics
data.frame(precision, recall, f1)
```

```{r}
# C5.0
tree5 <- C5.0(UNS ~., data=training_data)
fit5 = predict(tree5, test_data)
table = table(fit5, test_data$UNS)
table


n = sum(table) # number of instances
nc = nrow(table) # number of classes
diag = diag(table) # number of correctly classified instances per class 
rowsums = apply(table, 1, sum) # number of instances per class
colsums = apply(table, 2, sum) # number of predictions per class
p = rowsums / n # distribution of instances over the actual classes
q = colsums / n # distribution of instances over the predicted classes


precision = diag / colsums 
recall = diag / rowsums 
f1 = 2 * precision * recall / (precision + recall)

#The accuracy is:
accuracy = sum(diag) / n
accuracy

#Here is the performance metrics
data.frame(precision, recall, f1)
table(fit5, test_data$UNS)
```
```{r}
# naive bayes
bayes <- naiveBayes(UNS ~., data=training_data)
fit6 = predict(bayes, test_data)
table = table(fit6, test_data$UNS)

table


n = sum(table) # number of instances
nc = nrow(table) # number of classes
diag = diag(table) # number of correctly classified instances per class 
rowsums = apply(table, 1, sum) # number of instances per class
colsums = apply(table, 2, sum) # number of predictions per class
p = rowsums / n # distribution of instances over the actual classes
q = colsums / n # distribution of instances over the predicted classes


precision = diag / colsums 
recall = diag / rowsums 
f1 = 2 * precision * recall / (precision + recall)

#The accuracy is:
accuracy = sum(diag) / n
accuracy

#Here is the performance metrics
data.frame(precision, recall, f1)
table(fit5, test_data$UNS)

# Aggregate Data - Add accuracy to each model, compile, add missing algorithms if possible
```

```{r}
# knn
nn4 <- kNN(UNS ~ .,training_data,test_data,norm=FALSE,k=4)
table = table(test_data[,'UNS'],nn4)

table


n = sum(table) # number of instances
nc = nrow(table) # number of classes
diag = diag(table) # number of correctly classified instances per class 
rowsums = apply(table, 1, sum) # number of instances per class
colsums = apply(table, 2, sum) # number of predictions per class
p = rowsums / n # distribution of instances over the actual classes
q = colsums / n # distribution of instances over the predicted classes


precision = diag / colsums 
recall = diag / rowsums 
f1 = 2 * precision * recall / (precision + recall)

#The accuracy is:
accuracy = sum(diag) / n
accuracy

#Here is the performance metrics
data.frame(precision, recall, f1)
table(fit5, test_data$UNS)
```

```{r}
# SVM classification
model <- svm( UNS~., training_data )
res <- predict( model, test_data )
table = table(res, test_data$UNS)

table


n = sum(table) # number of instances
nc = nrow(table) # number of classes
diag = diag(table) # number of correctly classified instances per class 
rowsums = apply(table, 1, sum) # number of instances per class
colsums = apply(table, 2, sum) # number of predictions per class
p = rowsums / n # distribution of instances over the actual classes
q = colsums / n # distribution of instances over the predicted classes


precision = diag / colsums 
recall = diag / rowsums 
f1 = 2 * precision * recall / (precision + recall)

#The accuracy is:
accuracy = sum(diag) / n
accuracy

#Here is the performance metrics
data.frame(precision, recall, f1)
table(fit5, test_data$UNS)
```
